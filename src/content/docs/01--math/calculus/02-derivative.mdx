---
title: ë¯¸ë¶„ (Derivative)
description: ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„ ì›ë¦¬. ì»´í“¨í„° ê³¼í•™ì—ì„œì˜ ì‘ìš© ë§›ë³´ê¸°.
keywords: [ë¯¸ë¶„, ë„í•¨ìˆ˜, ë‹¤í•­í•¨ìˆ˜, ê¸°ìš¸ê¸°, ë¯¸ë¶„ê³„ìˆ˜,
           differentiation, derivative, polynomial function]
sidebar:
    order: 2
    badge:
        text: Î²
        variant: caution
lastUpdated: 2025-09-06
---

import { Aside, Card, Tabs, TabItem } from '@astrojs/starlight/components';

<Aside type="tip">
**ë¯¸ë¶„ì´ë€?** í•¨ìˆ˜ì˜ ìˆœê°„ ë³€í™”ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ê°œë…ìœ¼ë¡œ, ì–´ë–¤ ì ì—ì„œ í•¨ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ë³€í•˜ëŠ”ì§€ ì¸¡ì •í•œë‹¤.  
ê¸°í•˜í•™ì ìœ¼ë¡œëŠ” ê³¡ì„  ìœ„ì˜ í•œ ì ì—ì„œ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤.
</Aside>

<Aside>
ì˜ì–´ì—ì„œ "derivative"ì™€ "differential"ì€ ì„œë¡œ ë‹¤ë¥¸ ê°œë…ì´ë‹¤:
- **Derivative**: ë¯¸ë¶„ê³„ìˆ˜ ë˜ëŠ” ë„í•¨ìˆ˜ë¥¼ ì˜ë¯¸[^1]
- **Differential**: í•¨ìˆ˜ì˜ ë¯¸ì†Œ(infinitesimal) ë³€í™”ëŸ‰ì„ ì˜ë¯¸[^2]

ì´ ë¬¸ì„œëŠ” "derivative"ì— ì¤‘ì ì„ ë‘ì–´ ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì—„ë°€íˆ ì¦ëª…í•˜ê³  ì‘ìš©ì„ ë‹¤ë£¬ë‹¤.
</Aside>

## ê°œìš”

ë¯¸ë¶„ì€ í˜„ëŒ€ ê³¼í•™ê³¼ ê³µí•™ì˜ í•µì‹¬ ë„êµ¬ì´ë‹¤. íŠ¹íˆ ì»´í“¨í„° ê³¼í•™ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™œìš©í•œë‹¤:
- **ìµœì í™”**: ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì˜ ê¸°ì´ˆ
- **ë¨¸ì‹ ëŸ¬ë‹**: ì—­ì „íŒŒ(Backpropagation) ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬
- **ì»´í“¨í„° ê·¸ë˜í”½ìŠ¤**: ê³¡ì„ ê³¼ í‘œë©´ì˜ ë¶€ë“œëŸ¬ìš´ ë Œë”ë§
- **ìˆ˜ì¹˜í•´ì„**: ë¯¸ë¶„ë°©ì •ì‹ì˜ ìˆ˜ì¹˜ì  í•´ë²•

## ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„ ì¦ëª…
> ***Proof of Polynomial Differentiation***

### ê¸°ë³¸ ì •ì˜

í•¨ìˆ˜ $f: \mathbb{R} \to \mathbb{R}$ê°€ ì  $x$ì—ì„œ ë¯¸ë¶„ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¡°ê±´ì€ ë‹¤ìŒ ê·¹í•œì´ ì¡´ì¬í•¨ì´ë‹¤:

$$
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

ì´ ê°’ì„ $x$ì—ì„œì˜ **ë„í•¨ìˆ˜(derivative)** ë˜ëŠ” **ë¯¸ë¶„ê³„ìˆ˜**ë¼ í•œë‹¤.

### ë‹¨ê³„ 1: ìƒìˆ˜í•¨ìˆ˜ì˜ ë¯¸ë¶„

**ì •ë¦¬**: $f(x) = c$ (ìƒìˆ˜)ì¼ ë•Œ, $f'(x) = 0$

**ì¦ëª…**:
$$
f'(x) = \lim_{h \to 0} \frac{c - c}{h} = \lim_{h \to 0} \frac{0}{h} = 0
$$

<Aside type="note">
ìƒìˆ˜í•¨ìˆ˜ëŠ” ë³€í™”ê°€ ì—†ìœ¼ë¯€ë¡œ ë³€í™”ìœ¨ì´ 0ì´ë‹¤. ê·¸ë˜í”„ìƒìœ¼ë¡œëŠ” ìˆ˜í‰ì„ ì´ë©°, ê¸°ìš¸ê¸°ê°€ 0ì´ë‹¤.
</Aside>

### ë‹¨ê³„ 2: ë©±í•¨ìˆ˜ì˜ ë¯¸ë¶„ (Power Rule)

**ì •ë¦¬**: $f(x) = x^n$ (ë‹¨, $n \in \mathbb{N}$)ì¼ ë•Œ, $f'(x) = nx^{n-1}$

**ì¦ëª…**:
$$
f'(x) = \lim_{h \to 0} \frac{(x+h)^n - x^n}{h}
$$

ì´í•­ì •ë¦¬ë¥¼ ì ìš©í•˜ë©´:
$$
(x+h)^n = \sum_{k=0}^{n} \binom{n}{k} x^{n-k} h^k
$$

ì—¬ê¸°ì„œ $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ëŠ” ì´í•­ê³„ìˆ˜ì´ë‹¤.

ì „ê°œí•˜ë©´:
$$
(x+h)^n = x^n + nx^{n-1}h + \binom{n}{2}x^{n-2}h^2 + \cdots + h^n
$$

ë”°ë¼ì„œ:
$$
\begin{align}
f'(x) &= \lim_{h \to 0} \frac{x^n + nx^{n-1}h + \binom{n}{2}x^{n-2}h^2 + \cdots + h^n - x^n}{h} \\
&= \lim_{h \to 0} \left(nx^{n-1} + \binom{n}{2}x^{n-2}h + \cdots + h^{n-1}\right) \\
&= nx^{n-1}
\end{align}
$$

ë§ˆì§€ë§‰ ë“±ì‹ì€ $h \to 0$ì¼ ë•Œ $h$ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  í•­ì´ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

### ë‹¨ê³„ 3: ì„ í˜•ì„± (Linearity of Differentiation)

#### 3.1 í•©ì˜ ë²•ì¹™ (Sum Rule)

**ì •ë¦¬**: $(f + g)'(x) = f'(x) + g'(x)$

**ì¦ëª…**:
$$
\begin{align}
(f + g)'(x) &= \lim_{h \to 0} \frac{[f(x+h) + g(x+h)] - [f(x) + g(x)]}{h} \\
&= \lim_{h \to 0} \left[\frac{f(x+h) - f(x)}{h} + \frac{g(x+h) - g(x)}{h}\right] \\
&= \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} + \lim_{h \to 0} \frac{g(x+h) - g(x)}{h} \\
&= f'(x) + g'(x)
\end{align}
$$

#### 3.2 ìƒìˆ˜ë°°ì˜ ë²•ì¹™ (Constant Multiple Rule)

**ì •ë¦¬**: $(cf)'(x) = c \cdot f'(x)$ (ë‹¨, $c$ëŠ” ìƒìˆ˜)

**ì¦ëª…**:
$$
\begin{align}
(cf)'(x) &= \lim_{h \to 0} \frac{c \cdot f(x+h) - c \cdot f(x)}{h} \\
&= c \cdot \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \\
&= c \cdot f'(x)
\end{align}
$$

### ë‹¨ê³„ 4: ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„

**ì •ë¦¬**: ë‹¤í•­í•¨ìˆ˜ $P(x) = \sum_{k=0}^{n} a_k x^k$ì— ëŒ€í•´:
$$
P'(x) = \sum_{k=1}^{n} k \cdot a_k x^{k-1}
$$

**ì¦ëª…**: ìœ„ì˜ ë²•ì¹™ë“¤ì„ ì¡°í•©í•˜ë©´:
$$
\begin{align}
P'(x) &= \frac{d}{dx}\left(\sum_{k=0}^{n} a_k x^k\right) \\
&= \sum_{k=0}^{n} \frac{d}{dx}(a_k x^k) \quad \text{(í•©ì˜ ë²•ì¹™)} \\
&= \sum_{k=0}^{n} a_k \cdot \frac{d}{dx}(x^k) \quad \text{(ìƒìˆ˜ë°° ë²•ì¹™)} \\
&= \sum_{k=0}^{n} a_k \cdot k x^{k-1} \quad \text{(ë©±ì˜ ë²•ì¹™)} \\
&= \sum_{k=1}^{n} k \cdot a_k x^{k-1}
\end{align}
$$

ë§ˆì§€ë§‰ ë“±ì‹ì—ì„œ $k=0$ í•­ì€ 0ì´ë¯€ë¡œ í•©ì—ì„œ ì œì™¸í•œë‹¤.

## ì»´í“¨í„° ê³¼í•™ì—ì„œì˜ ì‘ìš©

### 1. ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)

ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì†ì‹¤í•¨ìˆ˜ $L(\theta)$ë¥¼ ìµœì†Œí™”í•  ë•Œ ë¯¸ë¶„ì„ í™œìš©í•œë‹¤:

<Tabs>
<TabItem label="Python">
```python
def gradient_descent(f, df, x0, learning_rate=0.01, iterations=100):
    """
    f: ëª©ì í•¨ìˆ˜
    df: fì˜ ë„í•¨ìˆ˜
    x0: ì´ˆê¸°ê°’
    """
    x = x0
    history = [x]
    
    for _ in range(iterations):
        grad = df(x)  # ë¯¸ë¶„ ê³„ì‚°
        x = x - learning_rate * grad  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        history.append(x)
    
    return x, history

# ì˜ˆì‹œ: f(x) = x^2 - 4x + 4 ìµœì†Œí™”
def f(x): return x**2 - 4*x + 4
def df(x): return 2*x - 4  # ë„í•¨ìˆ˜

minimum, path = gradient_descent(f, df, x0=0)
print(f"ìµœì†Ÿê°’ ìœ„ì¹˜: {minimum}")  # 2ì— ìˆ˜ë ´
```
</TabItem>

<TabItem label="JavaScript">
```javascript
function gradientDescent(f, df, x0, learningRate = 0.01, iterations = 100) {
    let x = x0;
    const history = [x];
    
    for (let i = 0; i < iterations; i++) {
        const grad = df(x);  // ë¯¸ë¶„ ê³„ì‚°
        x = x - learningRate * grad;  // íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        history.push(x);
    }
    
    return { minimum: x, history };
}

// ì˜ˆì‹œ: f(x) = x^2 - 4x + 4 ìµœì†Œí™”
const f = x => x**2 - 4*x + 4;
const df = x => 2*x - 4;  // ë„í•¨ìˆ˜

const result = gradientDescent(f, df, 0);
console.log(`ìµœì†Ÿê°’ ìœ„ì¹˜: ${result.minimum}`);  // 2ì— ìˆ˜ë ´
```
</TabItem>
</Tabs>

### 2. ìë™ ë¯¸ë¶„ (Automatic Differentiation)

í˜„ëŒ€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ëŠ” ìë™ìœ¼ë¡œ ë„í•¨ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤:

```python
import torch

# PyTorchì˜ ìë™ ë¯¸ë¶„
x = torch.tensor([2.0], requires_grad=True)
y = x**3 - 2*x**2 + x - 1  # f(x) = xÂ³ - 2xÂ² + x - 1

y.backward()  # ìë™ìœ¼ë¡œ dy/dx ê³„ì‚°
print(f"x=2ì—ì„œì˜ ë„í•¨ìˆ˜: {x.grad}")  # f'(2) = 3(2)Â² - 4(2) + 1 = 5
```

### 3. ìˆ˜ì¹˜ ë¯¸ë¶„ vs í•´ì„ì  ë¯¸ë¶„

<Card title="ë¹„êµ" icon="information">
| ë°©ë²•          | ì¥ì                  | ë‹¨ì                      | ì‹œê°„ë³µì¡ë„ |
| ------------ | ------------------- | ----------------------- | ------- |
| **ìˆ˜ì¹˜ ë¯¸ë¶„**  | êµ¬í˜„ ê°„ë‹¨, ëª¨ë“  í•¨ìˆ˜ ê°€ëŠ¥ | ì˜¤ì°¨ ë°œìƒ, ê³„ì‚° ë¹„ìš© ë†’ìŒ    |  $O(n)$  |
| **í•´ì„ì  ë¯¸ë¶„** | ì •í™•, ë¹ ë¦„            | ìˆ˜ì‹ í•„ìš”, ë³µì¡í•œ í•¨ìˆ˜ ì–´ë ¤ì›€ |  $O(1)$  |
| **ìë™ ë¯¸ë¶„**  | ì •í™•, ë³µì¡í•œ í•¨ìˆ˜ ê°€ëŠ¥   | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€          |  $O(1)$  |
</Card>

### 4. ì‹¤ì œ êµ¬í˜„: ë‹¤í•­ì‹ í´ë˜ìŠ¤

```python
class Polynomial:
    """ë‹¤í•­ì‹ê³¼ ê·¸ ë„í•¨ìˆ˜ë¥¼ ë‹¤ë£¨ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, coefficients):
        """
        coefficients: [a0, a1, a2, ...] for a0 + a1*x + a2*x^2 + ...
        """
        self.coeffs = coefficients
    
    def __call__(self, x):
        """ë‹¤í•­ì‹ ê°’ ê³„ì‚°"""
        return sum(coeff * x**i for i, coeff in enumerate(self.coeffs))
    
    def derivative(self):
        """ë„í•¨ìˆ˜ ë°˜í™˜"""
        if len(self.coeffs) <= 1:
            return Polynomial([0])
        
        new_coeffs = [i * coeff for i, coeff in enumerate(self.coeffs)][1:]
        return Polynomial(new_coeffs)
    
    def __str__(self):
        terms = []
        for i, coeff in enumerate(self.coeffs):
            if coeff != 0:
                if i == 0:
                    terms.append(str(coeff))
                elif i == 1:
                    terms.append(f"{coeff}x")
                else:
                    terms.append(f"{coeff}x^{i}")
        return " + ".join(terms) if terms else "0"

# ì‚¬ìš© ì˜ˆì‹œ
p = Polynomial([1, -2, 3])  # 1 - 2x + 3x^2
print(f"P(x) = {p}")
print(f"P'(x) = {p.derivative()}")  # -2 + 6x
print(f"P(2) = {p(2)}")  # 9
print(f"P'(2) = {p.derivative()(2)}")  # 10
```

## ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### ì¼ë°˜í™”ì™€ í™•ì¥

ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¬ ë©±ì˜ ë²•ì¹™ì€ ì–‘ì˜ ì •ìˆ˜ $n$ì— ëŒ€í•´ ì¦ëª…í•˜ì˜€ë‹¤. ì´ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™•ì¥í•  ìˆ˜ ìˆë‹¤:

1. **ìŒì˜ ì •ìˆ˜ ì§€ìˆ˜**: $x^{-n} = \frac{1}{x^n}$ì— ëŒ€í•´ ì—°ì‡„ë²•ì¹™ ì ìš©
2. **ìœ ë¦¬ìˆ˜ ì§€ìˆ˜**: $x^{p/q}$ì— ëŒ€í•´ ìŒí•¨ìˆ˜ ë¯¸ë¶„ ì ìš©
3. **ì‹¤ìˆ˜ ì§€ìˆ˜**: ì§€ìˆ˜í•¨ìˆ˜ì™€ ë¡œê·¸í•¨ìˆ˜ë¥¼ í†µí•œ ì¼ë°˜í™”

### ê³„ì‚° ë³µì¡ë„ ê´€ì 

ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê³„ìˆ˜ë§Œ ì¡°ì •í•˜ë¯€ë¡œ $O(n)$ ì‹œê°„ì— ê³„ì‚°í•œë‹¤. ì´ëŠ” ìˆ˜ì¹˜ ë¯¸ë¶„ì˜ $O(n^2)$ë³´ë‹¤ íš¨ìœ¨ì ì´ë‹¤.

## ì°¸ê³  ìë£Œ

- ğŸ“º [3Blue1Brown: Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
- ğŸ“ [Automatic Differentiation in Machine Learning: a Survey](https://arxiv.org/abs/1502.05767)
- ğŸ“– [Deep Learning Book - Chapter 4: Numerical Computation](https://www.deeplearningbook.org/contents/numerical.html)

---

[^1]: [Wikipedia | "Derivative"](https://en.wikipedia.org/wiki/Derivative#:~:text=Derivatives%20can%20be%20generalized%20to%20functions%20of%20several%20real%20variables)  
"*The derivative â€¦ is the slope of the tangent line â€¦*"  
"*Derivatives can be generalized to functions â€¦*"

[^2]: [Wikipedia | "Differential (mathematics)"](https://en.wikipedia.org/wiki/Differential_(mathematics)#:~:text=The%20term%20differential%20is%20used%20nonrigorously%20in%20calculus%20to%20refer%20to%20an%20infinitesimal%20(%22infinitely%20small%22)%20change%20in%20some%20varying%20quantity)  
"*â€¦ refer to an infinitesimal ("infinitely small") change â€¦*"
