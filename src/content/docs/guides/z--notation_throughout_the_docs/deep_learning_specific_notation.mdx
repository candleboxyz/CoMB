---
title: Deep Learning 분야 표기법 정리
description: 딥러닝에서 자주 사용되는 표기법과 기호에 대한 가이드
draft: true
sidebar:
    badge:
        text: beta
        variant: caution
---

## 1. 신경망 구조 표기법

### 1.1 층(Layer) 인덱싱
- **$W^{(l)}$ 또는 $W^{[l]}$**: $l$번째 층의 가중치 행렬
  - 예: $W^{(2)}$는 2번째 층의 가중치
  - Andrew Ng는 $W^{[l]}$를, 많은 논문은 $W^{(l)}$를 선호
  
- **$b^{(l)}$**: $l$번째 층의 편향 벡터
- **$a^{(l)}$**: $l$번째 층의 활성화값 (activation)
- **$z^{(l)}$**: $l$번째 층의 선형 결합값 (활성화 전)
  - 관계: $z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$, $a^{(l)} = g^{(l)}(z^{(l)})$

### 1.2 노드/뉴런 인덱싱
- **$w_{ij}^{(l)}$**: $l$번째 층에서 노드 $i$에서 노드 $j$로의 가중치
  - 주의: 일부 문헌은 $w_{ji}$로 표기 (from $j$ to $i$)
- **$a_j^{(l)}$**: $l$번째 층의 $j$번째 노드의 활성화값

### 1.3 데이터 인덱싱
- **$x^{(i)}$**: $i$번째 훈련 샘플
- **$y^{(i)}$**: $i$번째 훈련 샘플의 레이블
- **$(x^{(i)}, y^{(i)})$**: $i$번째 훈련 쌍
- **$\hat{y}^{(i)}$ 또는 $\tilde{y}^{(i)}$**: $i$번째 샘플에 대한 예측값

### 1.4 미니배치 표기법
- **$X^{\{t\}}$**: $t$번째 미니배치
- **$m$**: 전체 훈련 샘플 수
- **$m^{\{t\}}$**: $t$번째 미니배치의 샘플 수
- **대문자 표기**: 배치를 나타낼 때 사용
  - $X \in \mathbb{R}^{n_x \times m}$: 입력 배치
  - $Y \in \mathbb{R}^{n_y \times m}$: 출력 배치

## 2. 학습 관련 표기법

### 2.1 손실 함수와 비용 함수
- **$\mathcal{L}(\hat{y}, y)$**: 단일 샘플에 대한 손실 함수 (Loss function)
- **$J(W, b)$ 또는 $\mathcal{J}(\theta)$**: 전체 데이터셋에 대한 비용 함수 (Cost function)
  - 관계: $J = \frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||W||^2$

### 2.2 경사하강법 표기
- **$\alpha$ 또는 $\eta$**: 학습률 (learning rate)
- **$\nabla_W J$ 또는 $\frac{\partial J}{\partial W}$**: 가중치에 대한 그래디언트
- **$\delta^{(l)}$**: $l$번째 층의 오차 신호 (역전파에서)
  - 정의: $\delta^{(l)} = \frac{\partial J}{\partial z^{(l)}}$

### 2.3 정규화 표기
- **$\lambda$**: L2 정규화 계수
- **$||W||^2$ 또는 $||W||_2^2$**: L2 노름의 제곱
- **$||W||_1$**: L1 노름
- **$||W||_F$**: Frobenius 노름 (행렬용)

## 3. 연산 관련 표기법

### 3.1 활성화 함수
- **$\sigma(z)$**: 시그모이드 함수 = $\frac{1}{1 + e^{-z}}$
- **$g(z)$**: 일반적인 활성화 함수
- **$\text{ReLU}(z)$ 또는 $\max(0, z)$**: Rectified Linear Unit
- **$\tanh(z)$**: 하이퍼볼릭 탄젠트

### 3.2 특수 연산
- **$\odot$**: 요소별 곱셈 (Hadamard product, element-wise multiplication)
  - 예: $a \odot b = [a_1b_1, a_2b_2, ..., a_nb_n]^T$
- **$\otimes$**: 크로네커 곱 (Kronecker product) 또는 외적
- **$*$**: 합성곱 (Convolution)
- **$\circledast$**: 순환 합성곱 (Circular convolution)

### 3.3 통계 연산
- **$\mathbb{E}[X]$ 또는 $\mathbb{E}_X[f(X)]$**: 기댓값
- **$\text{Var}(X)$ 또는 $\mathbb{V}[X]$**: 분산
- **$\mathcal{N}(\mu, \sigma^2)$**: 정규분포
- **$\sim$**: "~를 따른다" (예: $X \sim \mathcal{N}(0, 1)$)

## 4. 특수 기호와 인덱싱

### 4.1 차원 표기
- **$n^{[l]}$ 또는 $n_l$**: $l$번째 층의 뉴런 수
- **$n_x$**: 입력 특징의 차원
- **$n_y$**: 출력의 차원
- **$n_h$**: 은닉층 뉴런 수

### 4.2 시간 관련 (RNN/LSTM)
- **$x^{\langle t \rangle}$**: 시간 $t$에서의 입력
- **$h^{\langle t \rangle}$ 또는 $a^{\langle t \rangle}$**: 시간 $t$에서의 은닉 상태
- **$\overleftarrow{h}^{\langle t \rangle}$**: 역방향 RNN의 은닉 상태
- **$\overrightarrow{h}^{\langle t \rangle}$**: 순방향 RNN의 은닉 상태

### 4.3 CNN 관련
- **$f$**: 필터 크기 (filter size)
- **$s$**: 스트라이드 (stride)
- **$p$**: 패딩 (padding)
- **$n_C$**: 채널 수 (number of channels)
- **$*$**: 합성곱 연산

## 5. 자주 혼동되는 표기법들

### 5.1 전치(Transpose) 표기
- **$W^T$**: 행렬 $W$의 전치
- **$W^{(l)T}$ 또는 $(W^{(l)})^T$**: $l$번째 층 가중치의 전치
  - 주의: $W^{T(l)}$는 잘못된 표기

### 5.2 미분 표기
- **$\frac{\partial J}{\partial W}$**: 편미분 (스칼라 $J$를 행렬 $W$로 미분)
- **$\nabla_W J$**: 그래디언트 (벡터 표기)
- **$\delta_{ij}$**: 크로네커 델타 (Kronecker delta)
  - $$\delta_{ij} = \begin{cases} 1 & \text{if } i = j \\ 0 & \text{if } i \ne j \end{cases}$$

### 5.3 집합과 확률
- **$\mathcal{D}$**: 데이터셋
- **$\mathcal{X}$**: 입력 공간
- **$\mathcal{Y}$**: 출력 공간
- **$p(x)$ 또는 $p_X(x)$**: 확률 밀도/질량 함수
- **$p(y|x)$**: 조건부 확률

## 6. 최적화 알고리즘 표기

### 6.1 모멘텀 관련
- **$v$**: 속도 (velocity) 벡터
- **$\beta$ 또는 $\gamma$**: 모멘텀 계수
- **$v_{dW}^{(l)}$**: $W^{(l)}$에 대한 속도

### 6.2 Adam 최적화
- **$m$**: 1차 모멘트 추정 (평균)
- **$v$**: 2차 모멘트 추정 (분산)
- **$\beta_1, \beta_2$**: 지수 이동 평균 계수
- **$\epsilon$**: 수치 안정성을 위한 작은 값 (보통 $10^{-8}$)

## 7. 표기법 사용 예제

### 순전파 계산
```
z^{[1]} = W^{[1]}x + b^{[1]}
a^{[1]} = g^{[1]}(z^{[1]})
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}
a^{[2]} = g^{[2]}(z^{[2]})
```

### 역전파 계산
```
dz^{[2]} = a^{[2]} - y
dW^{[2]} = \frac{1}{m}dz^{[2]}(a^{[1]})^T
db^{[2]} = \frac{1}{m}\sum_{i=1}^m dz^{[2](i)}
da^{[1]} = (W^{[2]})^T dz^{[2]}
dz^{[1]} = da^{[1]} \odot g'^{[1]}(z^{[1]})
```

### 파라미터 업데이트
```
W^{[l]} := W^{[l]} - \alpha \frac{\partial J}{\partial W^{[l]}}
b^{[l]} := b^{[l]} - \alpha \frac{\partial J}{\partial b^{[l]}}
```

## 8. 표기법 선택 가이드

논문이나 코드를 작성할 때:
1. **일관성 유지**: 한 문서 내에서는 동일한 표기법 사용
2. **명확성 우선**: 모호할 수 있는 경우 더 명확한 표기 선택
3. **커뮤니티 관례 따르기**: 특정 분야의 표준 표기법 존중
4. **정의 명시**: 처음 사용하는 기호는 반드시 정의

## 부록: 그리스 문자 사용

딥러닝에서 자주 사용되는 그리스 문자:
- α (alpha): 학습률 (하지만 η가 더 보편적)
- β (beta): 모멘텀 계수, 베타 분포 파라미터
- γ (gamma): 할인 계수 (강화학습)
- δ (delta): 오차, 작은 변화량
- ε (epsilon): 작은 값, 탐험율 (ε-greedy)
- η (eta): 학습률
  - *수학에서는 종종 η로 '작은 양이지만 조절 가능한 step'을 표현*
- θ (theta): 모델 파라미터 벡터
- λ (lambda): 정규화 계수
- μ (mu): 평균
- σ (sigma): 표준편차, 시그모이드 함수
- τ (tau): 온도 파라미터, 시간 상수
- φ (phi): 특징 함수
- ω (omega): 각속도, 가중치 (w 대신)